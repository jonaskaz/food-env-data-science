---
title: "Food Environmental Predictors of County Level Diabetes Rates"
author: "Jonas Kazlauskas and Shawn Albertson"
date: "12/15/2020"
output:
  github_document:
    toc: true
---
## Introduction  

*talk about uncertainty in the data itself here?*
This dataset we used for our analysis is the Food Environment Atlas, taken from the USDA Economic Research Service. This datatset contains information on a number of food related environmental factors such as proximity to stores, food prices, food assistance programs, local food availability. It also includes data on factors that may be related to these such as obesity rate, diabetes rate, and physical activity. This data is provided on the country level across the United States. The purpose of the food environment atlas is to foster research in these areas, and give an overview of food access and health.

The data for the Food Environment Atlas is largely taken from CDC estimates. The CDC uses data from the Behavioral Risk Factor Surveillance System which is under the US Census Bureau to make estimates. 

Diabetes rate in this dataset is defined as the rate (percent) of persons over 20 years old with a BMI over 30 within each county. We compared this rate at the county level between other factors in this dataset such as proximity to grocery stores, food insecurity, socioeconomic factors and more to determine the best predictors.

## Our question
We set out to answer the question:  What are the best predictors of adult diabetes rate in the US?

## Initial EDA

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library("readxl")
library(glmnet)
library(broom)
library(forcats)
```

```{r load data}
path <- "data/FoodEnvironmentAtlas.xls"
# Create a list of dataframes from every sheet in the Food Environment Atlas
full_list <- path %>%
  excel_sheets() %>%
  set_names() %>%
  map(read_excel, path = path)

# Get updated county level diabetes rates for 2018
diabetes_path <- "data/diabetes2018.csv"
df_diabetes18 <- 
  read_csv(
    diabetes_path,
    skip = 2
  ) %>% 
  rename(
    "FIPS" = County_FIPS, 
    "diabetes18" = "Diagnosed Diabetes Percentage"
    ) %>% 
  select(c(FIPS, diabetes18)) %>% 
  mutate(FIPS = ifelse(nchar(FIPS) == 4, paste("0", FIPS, sep = ""), FIPS))

# Add new diabetes data to list of data frames
full_list[[length(full_list)]] <- df_diabetes18

# Create a modified list of the dataframes to only include ones with `FIPS` columns
fips_list <- full_list[c(3, 5:10, 12:length(full_list))]

# Create a single dataframe with all parameters for all counties
df_full_fips <- 
  fips_list %>% 
  reduce(inner_join, by = c("FIPS"))

# Remove columns from dataframe that are not valid predictors. In our case, we remove columns that are geographic in nature or contain data also describing diabetes.
df_full <- 
  df_full_fips %>% 
  select(-c("FIPS", matches("State"), matches("County"), "PCT_DIABETES_ADULTS08", "PCT_DIABETES_ADULTS13"))
```

The most obvious thing to notice from is the fact that this dataset includes a lot of different statistics. Specifically, there are 177 statistics reported for 3139 FIPS codes. We also looked at some summary statistics for `diabetes18` which contains the percentage rates of diabetes diagnosed in adults in each county in 2018. We can see that the mean diabetes rate is about 8.72%, which is slightly higher than the CDC's 2020 estimate of national diabetes rate at 10.2% [6]. A histogram allows us to visualize the distribution that diabetes rate takes across all counties. The distribution looks roughly lognormal, with the peak shifted below the mean. When we isolate the counties with the ten highest diabetes rates, we find a range of states, but three out of the ten listed are actually in South Dakota. 

```{r EDA}
df_full %>% 
  dim()

df_full_fips %>%
  select(diabetes18) %>% 
  summary()

df_full_fips %>% 
  select(c("County.x", "State.x", "diabetes18")) %>% 
  arrange(desc(diabetes18)) %>% 
  head(10)

df_full_fips %>%
  select(diabetes18) %>% 
  ggplot() +
  geom_histogram(aes(x = diabetes18), binwidth = .5, boundary = 10) +
  xlab("Percent of Adults Diagnosed with Diabetes")
```

## Methodology
In order to isolate the most important factors in a model to predict diabetes, we used a technique called lasso regression. Lasso regression is a form of **regularization**, which describes a family of techniques that help your model effectively balance **bias** and **variance**. In other words, a model should be optimized such that it is neither **underfit** (high bias) nor **overfit** (high variance). One way to overfit a model is to make coefficients which are too high, leading to a model which works very well for a training data but not as well on yet unseen data. On the other hand, eliminating too many features of a model may be oversimplified and have high bias.

We will use the `glmnet` package in R to do lasso regression. This package depends on creating a **design matrix** rather than inputting a dataframe directly, so this is where we start. We use function `model.matrix` to compute the design matrix for the linear model predicting `diabetes18` using every other column at every observed county. This function removes rows with `NA` values from the dataframe, so the result is somewhat shorter than the input. The use of `glmnet` also depends on creating a list of the values we want to predict.

```{r create design matrix}
# Create a matrix of all the predictors in df_full
# This also makes everything numeric and turns qualitative data into dummy vars.
x_vars <- model.matrix(diabetes18~. , df_full)

# Pull diabetes rates into a list
y_var <- df_full$diabetes18
```

Key to using lasso regression is splitting data into a training set and a testing set. The training set is used to inform the coefficients of a linear model, while the testing set is used to evaluate how well those coefficients work on data which is not used to develop the model itself.

```{r split data into test and train}
# Set sampling random seed
set.seed(4)

# Train is a list of indexes from where training data is taken
train = sample(1:nrow(x_vars), nrow(x_vars)/2)

training_data_x <- x_vars[train,]
training_data_y <- y_var[train]

# x_test is indexes where training data is NOT taken
x_test = (-train)

# y_test is values at testing indexes
y_test = y_var[x_test]
```

Lasso regression introduces a parameter `lambda` which penalizes high correlation coefficients. A higher lambda has the effect of *reducing* the dependence of an predicted variable The effect of lambda is to prevent overfitting by reducing the direct



```{r}
# setup a sequence of lambda values to go through.
lambda_seq <- 10^seq(2, -2, by = -.1)

```

```{r}
cv_output = cv.glmnet(training_data_x,
                   training_data_y,
                   alpha = 1,
                   lambda = lambda_seq) # Fit lasso model on training data
plot(cv_output) # Draw plot of training MSE as a function of lambda
cv_output
```

```{r}
# Create model with minimum lambda value
lambda_min <- cv_output$lambda.min
lasso_min <- glmnet(x_vars[train,], y_var[train], alpha = 1, lambda = lambda_min)
pred_min <- predict(lasso_min, s = lambda_min,  newx = x_vars[x_test,])

lasso_min %>% 
  tidy() %>%
  mutate(est_ab = abs(estimate)) %>%
  arrange(desc(est_ab))


# lasso_min %>% 
#   tidy() %>% 
#   arrange(desc(estimate)) %>% 
#   mutate(estimate = abs(estimate))

# # Create model with 1se lambda value
# lambda_1se <- cv_output$lambda.1se
# lasso_1se <- glmnet(x_vars[train,], y_var[train], alpha = 1, lambda = lambda_1se)
# pred_1se <- predict(lasso_1se, s = lambda_1se,  newx = x_vars[x_test,])
# 
# lasso_1se %>%
#   tidy() %>%
#   arrange(desc(estimate))

# Create dataframe with predictions from both models compared with actual values
# final <- cbind(y_var[x_test], pred_min)

# as.tibble(final) %>%
#   rename(
#     "predicted" = "1",
#     "actual" = "V1"
#     ) 
```



## What are the best predictors? - Shawn
*what they are, why that might be the case*

## How good is this model for predicting diabetes rates? 
*MSE or Rsquared values*- Shawn
*confidence interval (CI or PI?? (probably CI)) of lasso model* - Jonas

We noticed that our lasso model outputs varying correlation coefficients when ran multiple times. To account for this variation, we decided to perform a bootstrap to find a confidence interval for each of our correlation coefficients. We used the library HDCI to perform the bootstrap. The results are shown below, with any perdictor with intervals that included 0 removed to focus on our top predictors. We noticed that for larger estimates, their confidence interval range tended to be larger.

```{r}
library(HDCI)
CI = bootLasso(x_vars[train,], y_var[train], alpha = 0.05)
```

```{r echo=FALSE}
intervals <- 
  as.tibble(CI$interval) %>% 
  tidy() %>%
  filter(min != 0, 
         max != 0,
         min >= 0 | min <= 0 & max <= 0) %>%
  mutate(abs_est = abs(mean)) %>%
  arrange(desc(abs_est)) %>%
  tail(-1)

coef_conf <- lasso_best %>%
  tidy() %>%
  mutate(est_ab = abs(estimate)) %>%
  arrange(desc(est_ab)) %>%
  tail(-1) %>%
  head(-8)

est_intervals <- intervals %>%
  cbind(coef_conf)
```

```{r}
est_intervals %>% 
  select(
    term,
    estimate,
    est_ab,
    min,
    max
  )
```


*CI for the lambda itself(?)* - Shawn

## Plots - Jonas
*maybe here, maybe scattered throughout*
```{r}
est_intervals %>%
  mutate(term = fct_reorder(term, est_ab)) %>%
  ggplot(aes(term, est_ab)) +
  geom_errorbar(aes(ymin=est_ab -(range/2), ymax=est_ab +(range/2)), width=.1, color = "red") +
  geom_line() +
  geom_point() +
  theme_interval() +
  labs(title="Top Predictors of US Diabetes Rate",
       x = "Predictor",
       y = "Size of Correlation Coefficient")
```


```{r echo=FALSE}
theme_interval <- function() {
  theme_minimal() %+replace%
  theme(
    axis.text.x = element_text(size = 9, angle = 90),
    axis.text.y = element_text(size = 12),
    axis.title.x = element_text(margin = margin(4, 4, 4, 4), size = 16),
    axis.title.y = element_text(margin = margin(4, 4, 4, 4), size = 12, angle = 90),
    legend.title = element_text(size = 16),
    legend.text = element_text(size = 12),
    strip.text.x = element_text(size = 12),
    strip.text.y = element_text(size = 12),
    panel.grid.major = element_line(color = "grey90"),
    panel.grid.minor = element_line(color = "grey90"),
    aspect.ratio = 4 / 10,
    plot.margin = unit(c(t = +0, b = +0, r = +0, l = +0), "cm"),
    plot.title = element_text(size = 18),
    plot.title.position = "plot",
    plot.subtitle = element_text(size = 16),
    plot.caption = element_text(size = 12)
  )
}
```


```{r echo = FALSE}
final <- as.tibble(cbind(y_var[x_test], pred_min)) %>%
  rowid_to_column("Index") %>%
  rename('Actual' = V1, "Predicted" = "1") %>%
  arrange(Actual) %>%
  mutate(Index=factor(Index, levels=Index))

plot <- head(final, 100) %>%
  ggplot(aes(Index, Actual)) +
  geom_point() +
  geom_point(aes(Index, Predicted), color = "red") +
  geom_errorbar(aes(ymin = Actual, ymax=Predicted, color = "red")) +
  theme_error() +
  labs(title="Prediction Error of 100 samples",
       x = "Index",
       y = "Diabetes Rate ") +
  scale_color_manual(labels = c("Error"), values = c("red")) 
plot
  
```

```{r echo=FALSE}
theme_error <- function() {
  theme_minimal() %+replace%
  theme(
    axis.text.x = element_text(size = 5, angle = 90),
    axis.text.y = element_text(size = 12),
    axis.title.x = element_text(margin = margin(4, 4, 4, 4), size = 16),
    axis.title.y = element_text(margin = margin(4, 4, 4, 4), size = 12, angle = 90),
    legend.title = element_text(size = 16),
    legend.text = element_text(size = 12),
    strip.text.x = element_text(size = 12),
    strip.text.y = element_text(size = 12),
    panel.grid.major = element_line(color = "grey90"),
    panel.grid.minor = element_line(color = "grey90"),
    aspect.ratio = 4 / 10,
    plot.margin = unit(c(t = +0, b = +0, r = +0, l = +0), "cm"),
    plot.title = element_text(size = 18),
    plot.title.position = "plot",
    plot.subtitle = element_text(size = 16),
    plot.caption = element_text(size = 12)
  )
}
```


## Uncertainty
*address sources of uncertainty which are potentially outside the scope of what's already been discussed*
While we did account for uncertainty in our correlation coefficient calculation, there are still many sources uncertainty that we did not consider. The largest source is from the dataset itself. The values contained in the dataset are estimates themselves, and come with some level of uncertainty. For example, we found that the measure of food insecurity was estimated with a 90% confidence interval. This error reduces our certainty in our final results. 


## Conclusion - Shawn




## Sources
[1] https://www.ers.usda.gov/data-products/food-environment-atlas/data-access-and-documentation-downloads/#Current%20Version  
[2] https://www.ers.usda.gov/webdocs/DataFiles/80526/2017%20Food%20Environment%20Atlas%20Documentation.pdf?v=1143.5  
[3] https://rstatisticsblog.com/data-science-in-action/machine-learning/lasso-regression/  
[4] https://www.steveklosterman.com/over-under/  
[5] http://www.science.smith.edu/~jcrouser/SDS293/labs/lab10-r.html  
[6] https://www.cdc.gov/diabetes/pdfs/data/statistics/national-diabetes-statistics-report.pdf
[7] https://cran.r-project.org/web/packages/glmnet/glmnet.pdf
[8] https://www.pluralsight.com/guides/linear-lasso-and-ridge-regression-with-r


